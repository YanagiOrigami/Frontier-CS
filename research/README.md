# Research Problems

Real-world systems challenges requiring domain expertise in GPU computing, distributed systems, ML pipelines, databases, and security.

## Basic Usage

```bash
# List all problems
frontier-eval --list

# Evaluate a solution (requires Docker)
frontier-eval flash_attn <your_solution.py>

# Evaluate multiple problems
frontier-eval --problems flash_attn,cross_entropy <your_solution.py>
```

## Cloud Evaluation with SkyPilot

Some problems require GPUs or specific hardware. Use [SkyPilot](https://skypilot.readthedocs.io/) to run evaluations on cloud VMs.

**Setup:**

```bash
sky check
```

See [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html) for cloud credential setup.

**Usage:**

```bash
frontier-eval flash_attn <your_solution.py> --skypilot
```

## Batch Evaluation

For evaluating multiple solutions at once, create a pairs file mapping solutions to problems:

```
# pairs.txt format: solution_path:problem_id
solutions/my_flash_attn_v1.py:flash_attn
solutions/my_flash_attn_v2.py:flash_attn
solutions/my_cross_entropy.py:cross_entropy
```

Then run:

```bash
# Evaluate all pairs
frontier-eval batch --pairs-file pairs.txt

# Resume interrupted evaluation
frontier-eval batch --pairs-file pairs.txt --resume

# Check status
frontier-eval batch --status --results-dir results/batch
```

## Python API

```python
from frontier_cs import FrontierCSEvaluator

evaluator = FrontierCSEvaluator()

# Single problem
result = evaluator.evaluate("research", problem_id="flash_attn", code=my_code)
print(f"Score: {result.score}")

# With SkyPilot
result = evaluator.evaluate("research", problem_id="flash_attn", code=my_code,
                           backend="skypilot")

# Batch evaluation
results = evaluator.evaluate_batch("research",
                                  problem_ids=["flash_attn", "cross_entropy"],
                                  code=my_code)
```

## Problem Structure

Each problem is in its own directory under `research/problems/`:

```
research/problems/
├── flash_attn/           # Single problem
│   ├── config.yaml
│   ├── readme
│   ├── evaluator.py
│   └── resources/
├── gemm_optimization/    # Problem with variants
│   ├── squares/
│   ├── rectangles/
│   └── ...
└── ...
```

### File Reference

| File | Purpose |
|------|---------|
| `config.yaml` | Runtime config (Docker image, GPU requirement, timeout) |
| `readme` | Problem description, API spec, scoring formula |
| `set_up_env.sh` | Environment setup (install deps, check CUDA) |
| `download_datasets.sh` | Download datasets (for local pre-download) |
| `evaluate.sh` | Evaluation entry point |
| `run_evaluator.sh` | Invokes `evaluator.py` |
| `evaluator.py` | Core evaluation logic |
| `resources/` | Baseline code, benchmark, test data |

### config.yaml Example

```yaml
dependencies:
  uv_project: resources    # Optional: uv project in resources/
datasets: []               # Optional: dataset URLs
tag: hpc                   # Category: os, hpc, ai, db, pl, security
runtime:
  docker:
    image: andylizf/triton-tlx:tlx-nv-cu122
    gpu: true
  timeout_seconds: 1800
```

## Evaluation Flow

Inside the Docker container, the execution order is:

```
1. set_up_env.sh         →  Initialize environment
2. Copy solution.py      →  /work/execution_env/solution_env/
3. evaluate.sh           →  Check files, call run_evaluator.sh
4. run_evaluator.sh      →  python3 evaluator.py
5. evaluator.py          →  Load Solution.solve(), run benchmark, print score
```

The final score is extracted from the last numeric line of stdout.

## Solution Interface

Submit a `solution.py` implementing the `Solution` class. The interface varies by problem type:

### Triton Kernel Problems (flash_attn, cross_entropy, gemm_optimization...)

```python
class Solution:
    def solve(self, spec_path: str = None) -> dict:
        """
        Returns either:
        - {"code": "python_code_string"}
        - {"program_path": "path/to/kernel.py"}
        """
        kernel_code = '''
import triton
import triton.language as tl

@triton.jit
def my_kernel(...):
    ...

def entry_function(...):
    ...
'''
        return {"code": kernel_code}
```

### ML Training Problems (imagenet_pareto...)

```python
class Solution:
    def solve(self, train_loader, val_loader, metadata: dict) -> torch.nn.Module:
        """
        Train and return a model.

        metadata contains: num_classes, input_dim, param_limit,
                          baseline_accuracy, device, etc.
        """
        model = MyModel(...)
        # training loop
        return model
```

### Other Problems

Check each problem's `readme` for the specific `solve()` signature and return type.

## Generating Solutions with LLMs

Use `generate_solutions.py` to automatically generate solutions using LLMs like GPT-5, Claude, or Gemini.

### How It Works

The script generates a **Cartesian product** of problems × models:

- **Default behavior**: Uses all problems (from `problems.txt`) and all models (from `models.txt`)
- **Partial specification**: If you specify `--problem` or `--model`, only that dimension is filtered; the other uses all available options
- **Skips existing**: Already-generated solutions are skipped (use `--force` to regenerate)
- **Skips failures**: Failed generations are skipped and logged; run again to retry

**Examples:**
```bash
# All problems × all models (from models.txt)
python research/scripts/generate_solutions.py

# All problems × specific model
python research/scripts/generate_solutions.py --model gpt-5

# Specific problems × all models
python research/scripts/generate_solutions.py --problem "flash_attn"

# Specific problems × specific models (Cartesian product)
python research/scripts/generate_solutions.py --problem "gemm_*" --model gpt-5 claude-sonnet-4-5
# → generates: gpt5_gemm_squares, gpt5_gemm_rectangles, ..., claude_gemm_squares, ...
```

### Dry Run

Use `--dryrun` to preview what would be generated without actually calling the API:

```bash
python research/scripts/generate_solutions.py --problem "cant_be_late*" --model gpt-5 --dryrun
```

This shows the list of (problem, model) pairs that would be processed.

### Retrying Failed Generations

The script automatically skips failures and continues. To complete all generations:

```bash
# Run multiple times until all succeed
python research/scripts/generate_solutions.py --model gpt-5
python research/scripts/generate_solutions.py --model gpt-5  # retries failed ones
```

Failed generations are logged in `generation_logs/` for debugging.

### Basic Usage

```bash
# Generate solution for a single problem
python research/scripts/generate_solutions.py research/problems/flash_attn --model gpt-5

# Generate solutions for multiple problems using wildcards
python research/scripts/generate_solutions.py --problem "gemm_*" --model gpt-5

# Use multiple models
python research/scripts/generate_solutions.py --problem flash_attn --model gpt-5 claude-sonnet-4-5

# Dry run to preview what would be generated
python research/scripts/generate_solutions.py --problem "cant_be_late*" --model gpt-5 --dryrun
```

### Options

| Option | Description |
|--------|-------------|
| `--problem PATTERN` | Problem name pattern (supports wildcards), repeatable |
| `--problems-file FILE` | File containing problem directories |
| `--model MODEL [MODEL ...]` | Target model(s), e.g. `gpt-5`, `claude-sonnet-4-5`, `gemini-2.5-pro` |
| `--models-file FILE` | Newline-delimited model list |
| `--api-key KEY` | API key (or use env vars like `OPENAI_API_KEY`) |
| `--timeout SECONDS` | Request timeout (default: 600s) |
| `--temperature TEMP` | Sampling temperature (default: 0.7) |
| `--variants N` | Number of solutions per model (default: 1) |
| `--concurrency N` | Max parallel generations |
| `--force` | Regenerate existing solutions |
| `--dryrun` | Show what would be generated without running |

### Regenerating Existing Solutions

```bash
# Regenerate specific solutions
python research/scripts/generate_solutions.py --solution "gpt5_flash*" --model gpt-5

# From a solutions file
python research/scripts/generate_solutions.py --solutions-file solutions.txt
```

### Output

Generated solutions are saved to `solutions/{model}_{problem}/`:

```
solutions/
├── gpt5_flash_attn/
│   ├── config.yaml
│   ├── prepare_env.sh
│   ├── solve.sh
│   └── resources/
│       └── solution.py
└── gpt5_gemm_optimization_squares/
    └── ...
```

Generation logs are saved to `generation_logs/`.

### API Keys

Set API keys via environment variables:

```bash
export OPENAI_API_KEY=sk-...      # For GPT models
export ANTHROPIC_API_KEY=sk-...   # For Claude models
export GOOGLE_API_KEY=...         # For Gemini models
```

Or use `--api-key` / `--api-key-env` flags.
